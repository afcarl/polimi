{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbor\n",
    "k-Nearest neighbors classifier assigns the class of an example using the majority vote of the k most examples in the data. In this example, we apply k-nearest neighbor using the well-known Iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model, datasets, neighbors\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from matplotlib.colors import ListedColormap\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "target = np.array(iris.target)\n",
    "\n",
    "print(\"Number of examples: \", iris.data.shape[0])\n",
    "print(\"Number of variables:\", iris.data.shape[0])\n",
    "print(\"Variable names:     \", iris.feature_names)\n",
    "print(\"Target values:      \", iris.target_names)\n",
    "print(\"Class Distribution  \", [(x,sum(target==x)) for x in np.unique(target)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use only the first two variables (sepal length and sepal width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = iris.data[:, :2]\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we just evaluate the performance for k=15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = 15\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors=k)\n",
    "knn_eval = model_selection.cross_val_score(knn, x, y, cv=StratifiedKFold(n_splits=10,shuffle=True,random_state=1234))\n",
    "\n",
    "print(\"%d-nearest-neighbor   Accuracy=%.3f Std=%.3f\"%(k,np.average(knn_eval),np.std(knn_eval)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we perform an experiment to select the best k. For this purpose, we use the typical train-validation-test setup in which train-validation part is performed using cross validation and the final evaluation is done with the test set that was never used for selecting k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(x, y, test_size=0.33, random_state=1234, stratify=y)\n",
    "\n",
    "knn_accuracy = {}\n",
    "knn_std = {}\n",
    "\n",
    "best_k = -1\n",
    "best_accuracy = 0.0\n",
    "\n",
    "best_test_k = -1\n",
    "best_test_accuracy = 0.0\n",
    "\n",
    "plt_x_label = []\n",
    "plt_y_train = []\n",
    "plt_y_bar_train = []\n",
    "plt_y_test = []\n",
    "\n",
    "for k in np.arange(1,30,1):\n",
    "    knn = neighbors.KNeighborsClassifier(n_neighbors=k)\n",
    "    knn_eval = model_selection.cross_val_score(knn, x_train, y_train, cv=StratifiedKFold(n_splits=10,shuffle=True,random_state=1234))\n",
    "    #print(\"k-nn k=%3d Accuracy=%.3f Std=%.3f\"%(k,np.average(knn_eval),np.std(knn_eval)))\n",
    "    knn_accuracy[k] = np.average(knn_eval)\n",
    "    knn_std[k] = np.std(knn_eval)\n",
    "    \n",
    "    knn_model = neighbors.KNeighborsClassifier(n_neighbors=k)\n",
    "    knn_model = knn_model.fit(x_train, y_train)\n",
    "    knn_model_eval = knn_model.score(x_test, y_test)\n",
    "    \n",
    "    print(\"k-nn k=%3d Train %.3f +/- %.3f\\tTest %.3f\"%(k,np.average(knn_eval),np.std(knn_eval),np.average(knn_model_eval)))\n",
    "\n",
    "    \n",
    "    if np.average(knn_eval)>best_accuracy:\n",
    "        best_accuracy = np.average(knn_eval)\n",
    "        best_k = k\n",
    "\n",
    "    if np.average(knn_model_eval)>best_test_accuracy:\n",
    "        best_test_accuracy = np.average(knn_model_eval)\n",
    "        best_test_k = k\n",
    "    \n",
    "    plt_x_label = plt_x_label + [k]\n",
    "    plt_y_train = plt_y_train + [np.average(knn_eval)]\n",
    "    plt_y_test = plt_y_test + [np.average(knn_model_eval)]\n",
    "    plt_y_bar_train = plt_y_bar_train + [np.std(knn_eval)]\n",
    "    \n",
    "print(\"\\n\\nBest k=%d Accuracy on Train %.3f\"%(best_k,best_accuracy))\n",
    "print(\"Best k=%d Accuracy on Test  %.3f\"%(best_test_k,best_test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(1, figsize=(8, 6))\n",
    "font = {'family' : 'sans', 'size'   : 20}\n",
    "plt.rc('font', **font)\n",
    "plt.title('Train vs Test Accuracy')\n",
    "plt.plot(plt_x_label,plt_y_train)\n",
    "plt.plot(plt_x_label,plt_y_test)\n",
    "plt.ylim([0.5,1.0])\n",
    "\n",
    "\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Accuracy (%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(1, figsize=(8, 6))\n",
    "font = {'family' : 'sans', 'size'   : 20}\n",
    "plt.rc('font', **font)\n",
    "plt.ylim([0.5,1.0])\n",
    "plt.title('Train vs Test Accuracy (with std bars)')\n",
    "plt.plot(plt_x_label,plt_y_train)\n",
    "plt.plot(plt_x_label,plt_y_test)\n",
    "plt.errorbar(plt_x_label, plt_y_train, yerr=plt_y_bar_train, fmt='o')\n",
    "\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We repeat the same procedure using distance as the weight function for prediction, thus the class of more similar examples will weight more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_k = -1\n",
    "best_accuracy = 0.0\n",
    "\n",
    "best_test_k = -1\n",
    "best_test_accuracy = 0.0\n",
    "\n",
    "plt_x_label = []\n",
    "plt_y_train = []\n",
    "plt_y_bar_train = []\n",
    "plt_y_test = []\n",
    "\n",
    "for k in np.arange(1,30,1):\n",
    "    knn = neighbors.KNeighborsClassifier(n_neighbors=k,weights='distance')\n",
    "    knn_eval = model_selection.cross_val_score(knn, x_train, y_train, cv=StratifiedKFold(n_splits=10,shuffle=True,random_state=1234))\n",
    "    #print(\"k-nn k=%3d Accuracy=%.3f Std=%.3f\"%(k,np.average(knn_eval),np.std(knn_eval)))\n",
    "    knn_accuracy[k] = np.average(knn_eval)\n",
    "    knn_std[k] = np.std(knn_eval)\n",
    "    \n",
    "    knn_model = neighbors.KNeighborsClassifier(n_neighbors=k,weights='distance')\n",
    "    knn_model = knn_model.fit(x_train, y_train)\n",
    "    knn_model_eval = knn_model.score(x_test, y_test)\n",
    "    \n",
    "    print(\"k-nn k=%3d Train %.3f +/- %.3f\\tTest %.3f\"%(k,np.average(knn_eval),np.std(knn_eval),np.average(knn_model_eval)))\n",
    "\n",
    "    \n",
    "    if np.average(knn_eval)>best_accuracy:\n",
    "        best_accuracy = np.average(knn_eval)\n",
    "        best_k = k\n",
    "\n",
    "    if np.average(knn_model_eval)>best_test_accuracy:\n",
    "        best_test_accuracy = np.average(knn_model_eval)\n",
    "        best_test_k = k\n",
    "    \n",
    "    plt_x_label = plt_x_label + [k]\n",
    "    plt_y_train = plt_y_train + [np.average(knn_eval)]\n",
    "    plt_y_test = plt_y_test + [np.average(knn_model_eval)]\n",
    "    plt_y_bar_train = plt_y_bar_train + [np.std(knn_eval)]\n",
    "    \n",
    "print(\"\\n\\nBest k=%d Accuracy on Train %.3f\"%(best_k,best_accuracy))\n",
    "print(\"Best k=%d Accuracy on Test  %.3f\"%(best_test_k,best_test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(1, figsize=(8, 6))\n",
    "font = {'family' : 'sans', 'size'   : 20}\n",
    "plt.rc('font', **font)\n",
    "plt.ylim([0.5,1.0])\n",
    "plt.title('Train vs Test Accuracy (with std bars)')\n",
    "plt.plot(plt_x_label,plt_y_train);\n",
    "plt.plot(plt_x_label,plt_y_test);\n",
    "plt.errorbar(plt_x_label, plt_y_train, yerr=plt_y_bar_train, fmt='o');\n",
    "\n",
    "plt.xlabel('k');\n",
    "plt.ylabel('Accuracy (%)');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
